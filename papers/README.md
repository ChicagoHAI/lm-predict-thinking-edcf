# Downloaded Papers

1. [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](2201.11903_chain-of-thought.pdf)
   - Authors: Jason Wei, Xuezhi Wang, Dale Schuurmans, et al. (Google Brain)
   - Year: 2022 (arXiv:2201.11903)
   - Why relevant: Introduces chain-of-thought prompting; establishes the baseline notion of “thinking tokens” for multi-step reasoning.

2. [Self-Consistency Improves Chain of Thought Reasoning in Language Models](2203.11171_self-consistency.pdf)
   - Authors: Xuezhi Wang, Jason Wei, Dale Schuurmans, et al. (Google Brain)
   - Year: 2022/2023 (ICLR 2023; arXiv:2203.11171)
   - Why relevant: Proposes sampling multiple reasoning paths and voting—useful for comparing predicted vs. realized reasoning length.

3. [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](2205.10625_least-to-most.pdf)
   - Authors: Denny Zhou, Nathanael Schärli, Le Hou, et al. (Google Brain)
   - Year: 2022/2023 (ICLR 2023; arXiv:2205.10625)
   - Why relevant: Breaks problems into steps, varying chain length; supports experiments on forecasting reasoning tokens needed.

4. [ReAct: Synergizing Reasoning and Acting in Language Models](2210.03629_react.pdf)
   - Authors: Shunyu Yao, Jeffrey Zhao, Dian Yu, et al. (Princeton + Google)
   - Year: 2022/2023 (ICLR 2023; arXiv:2210.03629)
   - Why relevant: Interleaves reasoning tokens with actions; useful for measuring and predicting compute budgets per task.

5. [Reflexion: Language Agents with Verbal Reinforcement Learning](2303.11366_reflexion.pdf)
   - Authors: Noah Shinn, Federico Cassano, Edward Berman, et al. (Northeastern, MIT, Princeton)
   - Year: 2023 (arXiv:2303.11366)
   - Why relevant: Adds self-evaluation loops that alter reasoning length; applicable to predicting when extra thinking helps.

6. [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](2305.10601_tree-of-thought.pdf)
   - Authors: Shunyu Yao, Dian Yu, Jeffrey Zhao, et al. (Princeton + Google DeepMind)
   - Year: 2023 (arXiv:2305.10601)
   - Why relevant: Explores search over reasoning trees; provides controllable thinking-time budgets to benchmark prediction accuracy.
